\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}

% Page layout
\geometry{a4paper, margin=1in}

% Listings configuration
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    breaklines=true,
    frame=single,
    columns=flexible,
    captionpos=b
}

% Title
\title{Python Machine Learning and Statistical Tests Reference Sheet}
\author{Evan Yeager}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction}
This reference sheet provides Python implementations of common machine learning, regression, classification models, and statistical tests both from scratch and using libraries such as \texttt{scikit-learn}, \texttt{scipy}, and \texttt{statsmodels}. It serves as a quick guide for understanding and implementing these models and tests effectively.

\section{Working with Pandas}

Pandas is a powerful Python library for data manipulation and analysis. It provides data structures like \texttt{DataFrame} and \texttt{Series} that make handling structured data efficient and intuitive. This section covers essential Pandas functionalities, including data loading, inspection, manipulation, and transformation.

\subsection{Importing Pandas and Reading Data}

\paragraph{Explanation}
Before using Pandas, you need to import it. Pandas can read data from various file formats such as CSV, Excel, JSON, and SQL databases.

\begin{lstlisting}[caption=Importing Pandas]
import pandas as pd
\end{lstlisting}

\subsubsection{Reading CSV Files}

\paragraph{Using \texttt{read\_csv}}
\begin{lstlisting}[caption=Reading a CSV File with pandas]
# Read a CSV file into a DataFrame
df = pd.read_csv('data.csv')

# Specify delimiter if not comma
df = pd.read_csv('data.tsv', delimiter='\t')
\end{lstlisting}

\subsubsection{Reading Excel Files}

\paragraph{Using \texttt{read\_excel}}
\begin{lstlisting}[caption=Reading an Excel File with pandas]
# Read the first sheet of an Excel file
df = pd.read_excel('data.xlsx')

# Read a specific sheet by name
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')
\end{lstlisting}

\subsubsection{Reading JSON Files}

\paragraph{Using \texttt{read\_json}}
\begin{lstlisting}[caption=Reading a JSON File with pandas]
# Read a JSON file into a DataFrame
df = pd.read_json('data.json')
\end{lstlisting}

\subsection{Inspecting Data}

\paragraph{Explanation}
Inspecting data helps in understanding its structure, identifying missing values, and getting summary statistics.

\subsubsection{Viewing the DataFrame}

\paragraph{Using \texttt{head()} and \texttt{tail()}}
\begin{lstlisting}[caption=Viewing DataFrame with head() and tail()]
# Display the first 5 rows
print(df.head())

# Display the last 10  rows
print(df.tail(10))
\end{lstlisting}

\subsubsection{DataFrame Information}

\paragraph{Using \texttt{info()}}
\begin{lstlisting}[caption=DataFrame Information with info()]
# Get a concise summary of the DataFrame
df.info()
\end{lstlisting}

\subsubsection{Descriptive Statistics}

\paragraph{Using \texttt{describe()}}
\begin{lstlisting}[caption=Descriptive Statistics with describe()]
# Get summary statistics for numerical columns
print(df.describe())

# Include all columns (including non-numeric)
print(df.describe(include='all'))
\end{lstlisting}

\subsection{Selecting and Filtering Data}

\paragraph{Explanation}
Selecting specific rows and columns is fundamental for focused analysis. Pandas provides versatile methods like \texttt{loc}, \texttt{iloc}, and boolean indexing for this purpose.

\subsubsection{Selecting Columns}

\paragraph{Using Column Names}
\begin{lstlisting}[caption=Selecting Columns by Name]
# Select a single column as a Series
feature1 = df['Feature1']

# Select multiple columns as a DataFrame
subset = df[['Feature1', 'Feature2', 'Feature3']]
\end{lstlisting}

\subsubsection{Selecting Rows}

\paragraph{Using \texttt{loc} and \texttt{iloc}}
\begin{lstlisting}[caption=Selecting Rows with loc and iloc]
# Select rows by label using loc
subset = df.loc[10:20]  # Rows with index from 10 to 20

# Select rows by integer location using iloc
subset = df.iloc[0:5]    # First 5 rows
\end{lstlisting}

\subsubsection{Boolean Indexing}

\paragraph{Using Conditions}
\begin{lstlisting}[caption=Boolean Indexing with Conditions]
# Select rows where Feature1 > 50
filtered_df = df[df['Feature1'] > 50]

# Select rows with multiple conditions
filtered_df = df[(df['Feature1'] > 50) & (df['Feature2'] == 'CategoryA')]
\end{lstlisting}

\subsection{Handling Missing Data}

\paragraph{Explanation}
Missing data is common in real-world datasets. Pandas provides functions to detect, remove, or impute missing values.

\subsubsection{Detecting Missing Values}

\paragraph{Using \texttt{isnull()} and \texttt{notnull()}}
\begin{lstlisting}[caption=Detecting Missing Values]
# Check for missing values in each column
print(df.isnull().sum())

# Get a boolean mask of non-missing values
non_missing = df.notnull()
\end{lstlisting}

\subsubsection{Removing Missing Values}

\paragraph{Using \texttt{dropna()}}
\begin{lstlisting}[caption=Removing Missing Values with dropna()]
# Drop rows with any missing values
df_clean = df.dropna()

# Drop columns with any missing values
df_clean = df.dropna(axis=1)

# Drop rows where specific columns have missing values
df_clean = df.dropna(subset=['Feature1', 'Feature2'])
\end{lstlisting}

\subsubsection{Imputing Missing Values}

\paragraph{Using \texttt{fillna()}}
\begin{lstlisting}[caption=Imputing Missing Values with fillna()]
# Fill missing values with a constant
df_filled = df.fillna(0)

# Fill missing values with the mean of the column
df_filled = df.fillna(df['Feature1'].mean())

# Forward fill
df_filled = df.fillna(method='ffill')

# Backward fill
df_filled = df.fillna(method='bfill')
\end{lstlisting}

\subsection{Data Manipulation}

\paragraph{Explanation}
Manipulating data involves transforming, aggregating, and merging datasets to prepare them for analysis.

\subsubsection{Adding and Dropping Columns}

\paragraph{Adding a New Column}
\begin{lstlisting}[caption=Adding a New Column]
# Add a new column based on existing columns
df['Feature3'] = df['Feature1'] + df['Feature2']
\end{lstlisting}

\paragraph{Dropping Columns}
\begin{lstlisting}[caption=Dropping Columns with drop()]
# Drop a single column
df = df.drop('Feature3', axis=1)

# Drop multiple columns
df = df.drop(['Feature1', 'Feature2'], axis=1)
\end{lstlisting}

\subsubsection{Renaming Columns}

\paragraph{Using \texttt{rename()}}
\begin{lstlisting}[caption=Renaming Columns with rename()]
# Rename a single column
df = df.rename(columns={'Feature1': 'F1'})

# Rename multiple columns
df = df.rename(columns={'Feature1': 'F1', 'Feature2': 'F2'})
\end{lstlisting}

\subsubsection{Grouping Data}

\paragraph{Using \texttt{groupby()}}
\begin{lstlisting}[caption=Grouping Data with groupby()]
# Group by a categorical feature and calculate mean
grouped = df.groupby('Category')['Feature1'].mean()

# Group by multiple features and aggregate
grouped = df.groupby(['Category', 'Subcategory']).agg({
    'Feature1': 'mean',
    'Feature2': 'sum'
})
\end{lstlisting}

\subsubsection{Merging and Joining DataFrames}

\paragraph{Using \texttt{merge()}}
\begin{lstlisting}[caption=Merging DataFrames with merge()]
# Merge two DataFrames on a common key
merged_df = pd.merge(df1, df2, on='KeyColumn', how='inner')

# Merge with different key columns
merged_df = pd.merge(df1, df2, left_on='Key1', right_on='Key2', how='left')
\end{lstlisting}

\paragraph{Using \texttt{concat()}}
\begin{lstlisting}[caption=Concatenating DataFrames with concat()]
# Concatenate DataFrames vertically
combined_df = pd.concat([df1, df2, df3], axis=0)

# Concatenate DataFrames horizontally
combined_df = pd.concat([df1, df2, df3], axis=1)
\end{lstlisting}

\subsubsection{Pivot Tables}

\paragraph{Using \texttt{pivot\_table()}}
\begin{lstlisting}[caption=Creating Pivot Tables with pivot_table()]
# Create a pivot table
pivot = pd.pivot_table(df, values='Feature1', index='Category',
                       columns='Subcategory', aggfunc='mean', fill_value=0)
\end{lstlisting}

\subsection{Data Transformation}

\paragraph{Explanation}
Transforming data includes applying functions to modify data, normalizing, encoding categorical variables, and more.

\subsubsection{Applying Functions}

\paragraph{Using \texttt{apply()}}
\begin{lstlisting}[caption=Applying Functions with apply()]
# Apply a function to each column
df['Feature1'] = df['Feature1'].apply(lambda x: x * 2)

# Apply a function to each row
df['Feature_sum'] = df.apply(lambda row: row['Feature1'] + row['Feature2'], axis=1)
\end{lstlisting}

\subsubsection{Mapping Categorical Values}

\paragraph{Using \texttt{map()}}
\begin{lstlisting}[caption=Mapping Categorical Values with map()]
# Map categorical values to numerical
mapping = {'CategoryA': 1, 'CategoryB': 2, 'CategoryC': 3}
df['Category_num'] = df['Category'].map(mapping)
\end{lstlisting}

\subsubsection{Handling Duplicates}

\paragraph{Using \texttt{drop\_duplicates()}}
\begin{lstlisting}[caption=Handling Duplicates with drop_duplicates()]
# Drop duplicate rows
df = df.drop_duplicates()

# Drop duplicates based on specific columns
df = df.drop_duplicates(subset=['Feature1', 'Feature2'])
\end{lstlisting}

\subsubsection{Sorting Data}

\paragraph{Using \texttt{sort\_values()} and \texttt{sort\_index()}}
\begin{lstlisting}[caption=Sorting Data with sort_values() and sort_index()]
# Sort by a single column
df = df.sort_values(by='Feature1', ascending=False)

# Sort by multiple columns
df = df.sort_values(by=['Feature1', 'Feature2'], ascending=[True, False])

# Sort by index
df = df.sort_index()
\end{lstlisting}

\subsection{Handling Dates and Times}

\paragraph{Explanation}
Pandas offers robust support for date and time data, enabling easy manipulation and extraction of temporal features.

\subsubsection{Converting to Datetime}

\paragraph{Using \texttt{to\_datetime()}}
\begin{lstlisting}[caption=Converting to Datetime with to_datetime()]
# Convert a column to datetime
df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')

# Handle errors by coercing invalid parsing to NaT
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
\end{lstlisting}

\subsubsection{Extracting Date Components}

\paragraph{Using \texttt{dt accessor}}
\begin{lstlisting}[caption=Extracting Date Components with dt accessor]
# Extract year, month, day
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day

# Extract day of week
df['DayOfWeek'] = df['Date'].dt.day_name()
\end{lstlisting}

\subsection{Exporting Data}

\paragraph{Explanation}
After processing and analyzing data, you might want to save the results. Pandas supports exporting data to various formats.

\subsubsection{Writing to CSV}

\paragraph{Using \texttt{to\_csv()}}
\begin{lstlisting}[caption=Writing to CSV with to_csv()]
# Write DataFrame to a CSV file
df.to_csv('processed_data.csv', index=False)

# Write with a different delimiter
df.to_csv('processed_data.tsv', sep='\t', index=False)
\end{lstlisting}

\subsubsection{Writing to Excel}

\paragraph{Using \texttt{to\_excel()}}
\begin{lstlisting}[caption=Writing to Excel with to_excel()]
# Write DataFrame to an Excel file
df.to_excel('processed_data.xlsx', sheet_name='Sheet1', index=False)
\end{lstlisting}

\subsubsection{Writing to JSON}

\paragraph{Using \texttt{to\_json()}}
\begin{lstlisting}[caption=Writing to JSON with to_json()]
# Write DataFrame to a JSON file
df.to_json('processed_data.json', orient='records', lines=True)
\end{lstlisting}

\subsection{Advanced Data Operations}

\paragraph{Explanation}
For more complex data manipulation, Pandas provides advanced functionalities such as window functions, multi-indexing, and more.

\subsubsection{Pivoting and Melting}

\paragraph{Using \texttt{pivot()} and \texttt{melt()}}
\begin{lstlisting}[caption=Pivoting and Melting with pivot() and melt()]
# Pivot the DataFrame
pivot_df = df.pivot(index='Date', columns='Category', values='Feature1')

# Melt the DataFrame back to long format
melt_df = pivot_df.reset_index().melt(id_vars='Date', var_name='Category', value_name='Feature1')
\end{lstlisting}

\subsubsection{Window Functions}

\paragraph{Using \texttt{rolling()} and \texttt{expanding()}}
\begin{lstlisting}[caption=Window Functions with rolling() and expanding()]
# Calculate a rolling mean with window size 3
df['RollingMean'] = df['Feature1'].rolling(window=3).mean()

# Calculate an expanding sum
df['ExpandingSum'] = df['Feature1'].expanding().sum()
\end{lstlisting}

\subsubsection{Multi-Indexing}

\paragraph{Creating and Manipulating Multi-Index DataFrames}
\begin{lstlisting}[caption=Multi-Indexing with Pandas]
# Set multiple columns as index
df_multi = df.set_index(['Category', 'Subcategory'])

# Access data using multi-index
value = df_multi.loc['CategoryA', 'Subcategory1']['Feature1']

# Reset the index
df_reset = df_multi.reset_index()
\end{lstlisting}

\subsection{Best Practices for Using Pandas}

\begin{itemize}
    \item \textbf{Use Vectorized Operations:} Leverage Pandas' vectorized functions for efficient computation instead of iterating through rows.
    \item \textbf{Chain Methods Wisely:} Method chaining can make code more readable but ensure intermediate steps are clear.
    \item \textbf{Handle Missing Data Early:} Address missing values during the data cleaning phase to prevent issues in downstream analysis.
    \item \textbf{Optimize Memory Usage:} Specify data types when loading large datasets to reduce memory consumption.
    \item \textbf{Document Your Steps:} Keep track of data transformations for reproducibility and easier debugging.
\end{itemize}

\section{Linear Regression}
\subsection{Explanation}

Linear Regression is a foundational machine learning algorithm used for predicting a continuous target variable based on one or more input features. It assumes a linear relationship between the input variables and the target. The model learns coefficients for each feature to minimize the difference between the predicted and actual values, typically using methods like Ordinary Least Squares (OLS). 

\subsubsection{Assumptions}
\begin{itemize}
    \item \textbf{Linearity:} The relationship between the independent variables and the dependent variable is linear.
    \item \textbf{Independence:} Observations are independent of each other.
    \item \textbf{Homoscedasticity:} The residuals have constant variance at every level of the independent variables.
    \item \textbf{Normality:} The residuals of the model are normally distributed.
    \item \textbf{No Multicollinearity:} Independent variables are not highly correlated with each other.
\end{itemize}

\subsection{From Scratch}
\begin{lstlisting}[caption=Linear Regression from Scratch]
import numpy as np

class LinearRegression:
    def __init__(self, lr=0.01, epochs=1000):
        self.lr = lr
        self.epochs = epochs

    def fit(self, X, y):
        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term
        self.theta = np.zeros(X.shape[1])
        for _ in range(self.epochs):
            predictions = X.dot(self.theta)
            errors = predictions - y
            gradient = X.T.dot(errors) / len(y)
            self.theta -= self.lr * gradient

    def predict(self, X):
        X = np.c_[np.ones(X.shape[0]), X]
        return X.dot(self.theta)
\end{lstlisting}

\subsection{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Linear Regression with scikit-learn]
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
\end{lstlisting}

\section{Logistic Regression}
\subsection{Explanation}

Logistic Regression is a classification algorithm used to predict binary outcomes. Unlike Linear Regression, it models the probability that a given input belongs to a particular category using the logistic function. It is widely used in scenarios where the target variable is categorical, such as spam detection or disease diagnosis. Logistic Regression makes several assumptions to ensure accurate and reliable results:

\begin{enumerate}
    \item \textbf{Binary Outcome:} The dependent variable is binary.
    \item \textbf{Independence of Observations:} Observations are independent of each other.
    \item \textbf{No Multicollinearity:} Independent variables are not highly correlated with each other.
    \item \textbf{Linearity of Logit:} The log odds of the outcome are a linear combination of the independent variables.
    \item \textbf{Large Sample Size:} Logistic Regression requires a large sample size to provide reliable results.
\end{enumerate}

\subsection{From Scratch}
\begin{lstlisting}[caption=Logistic Regression from Scratch]
import numpy as np

class LogisticRegression:
    def __init__(self, lr=0.01, epochs=1000):
        self.lr = lr
        self.epochs = epochs

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        X = np.c_[np.ones(X.shape[0]), X]
        self.theta = np.zeros(X.shape[1])
        for _ in range(self.epochs):
            z = X.dot(self.theta)
            predictions = self.sigmoid(z)
            errors = y - predictions
            gradient = X.T.dot(errors) / len(y)
            self.theta += self.lr * gradient

    def predict_prob(self, X):
        X = np.c_[np.ones(X.shape[0]), X]
        return self.sigmoid(X.dot(self.theta))

    def predict(self, X, threshold=0.5):
        return self.predict_prob(X) >= threshold
\end{lstlisting}

\subsection{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Logistic Regression with scikit-learn]
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
probabilities = model.predict_proba(X_test)[:,1]
predictions = model.predict(X_test)
\end{lstlisting}

\section{Decision Trees}
\subsection{Explanation}

Decision Trees are versatile models used for both classification and regression tasks. They work by recursively splitting the dataset based on feature values that result in the most significant information gain or reduction in impurity. Decision Trees are easy to interpret but can be prone to overfitting, especially with deep trees. Decision Trees make the following assumptions:

\begin{enumerate}
    \item \textbf{Feature Independence:} Assumes that features are independent of each other.
    \item \textbf{No Assumptions About Data Distribution:} Unlike linear models, Decision Trees do not assume any underlying distribution of the data.
    \item \textbf{Handle Non-linear Relationships:} Capable of capturing non-linear relationships between features and the target.
    \item \textbf{No Need for Feature Scaling:} Decision Trees are not affected by the scale of the features.
    \item \textbf{Sufficient Data:} Requires a sufficient amount of data to make reliable splits, especially for deeper trees.
\end{enumerate}


\subsection{From Scratch}
\begin{lstlisting}[caption=Decision Tree from Scratch]
import numpy as np

class DecisionTree:
    def __init__(self, max_depth=5):
        self.max_depth = max_depth

    def fit(self, X, y):
        self.tree = self._build_tree(X, y)

    def _build_tree(self, X, y, depth=0):
        num_samples, num_features = X.shape
        if depth >= self.max_depth or len(set(y)) == 1:
            leaf_value = self._most_common_label(y)
            return {'type': 'leaf', 'value': leaf_value}
        
        best_feat, best_thresh = self._best_split(X, y)
        if best_feat is None:
            leaf_value = self._most_common_label(y)
            return {'type': 'leaf', 'value': leaf_value}

        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)
        left = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)
        right = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)
        return {'type': 'node', 'feature': best_feat, 'threshold': best_thresh, 'left': left, 'right': right}

    def _best_split(self, X, y):
        best_gain = -1
        split_idx, split_thresh = None, None
        for feat in range(X.shape[1]):
            thresholds = np.unique(X[:, feat])
            for thresh in thresholds:
                gain = self._information_gain(y, X[:, feat], thresh)
                if gain > best_gain:
                    best_gain = gain
                    split_idx = feat
                    split_thresh = thresh
        return split_idx, split_thresh

    def _information_gain(self, y, X_feat, thresh):
        parent_entropy = self._entropy(y)
        left_idxs, right_idxs = self._split(X_feat, thresh)
        if len(left_idxs) == 0 or len(right_idxs) == 0:
            return 0
        n = len(y)
        n_l, n_r = len(left_idxs), len(right_idxs)
        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])
        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r
        return parent_entropy - child_entropy

    def _split(self, X_feat, thresh):
        left_idxs = np.where(X_feat <= thresh)[0]
        right_idxs = np.where(X_feat > thresh)[0]
        return left_idxs, right_idxs

    def _entropy(self, y):
        hist = np.bincount(y)
        ps = hist / len(y)
        return -np.sum([p * np.log2(p) for p in ps if p > 0])

    def _most_common_label(self, y):
        return np.bincount(y).argmax()

    def predict(self, X):
        return np.array([self._traverse_tree(x, self.tree) for x in X])

    def _traverse_tree(self, x, node):
        if node['type'] == 'leaf':
            return node['value']
        if x[node['feature']] <= node['threshold']:
            return self._traverse_tree(x, node['left'])
        return self._traverse_tree(x, node['right'])
\end{lstlisting}

\subsection{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Decision Tree with scikit-learn]
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(max_depth=5)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
\end{lstlisting}

\section{Support Vector Machines (SVM)}
\subsection{Explanation}

Support Vector Machines are powerful classifiers that find the optimal hyperplane which maximizes the margin between different classes. They are effective in high-dimensional spaces and are versatile with different kernel functions (linear, polynomial, RBF) to handle non-linear separations. SVMs are robust against overfitting, especially in high-dimensional space. Support Vector Machines operate under the following assumptions:

\begin{enumerate}
    \item \textbf{Linearly Separable Data (for linear kernel):} Assumes that the data can be separated by a hyperplane.
    \item \textbf{Clear Margin of Separation:} Assumes that there is a clear margin between the classes.
    \item \textbf{No Overlapping Classes:} Assumes that the classes do not overlap significantly.
    \item \textbf{Independence of Features:} Assumes that features are independent.
    \item \textbf{High Dimensionality:} Performs well in high-dimensional spaces.
    \item \textbf{Effective Kernel Selection:} Assumes that an appropriate kernel is chosen to handle non-linear relationships.
\end{enumerate}

\subsection{From Scratch}
\begin{lstlisting}[caption=SVM from Scratch]
import numpy as np

class SVM:
    def __init__(self, lr=0.001, lambda_param=0.01, n_iters=1000):
        self.lr = lr
        self.lambda_param = lambda_param
        self.n_iters = n_iters
        self.w = None
        self.b = None

    def fit(self, X, y):
        y_ = np.where(y <= 0, -1, 1)
        n_samples, n_features = X.shape
        self.w = np.zeros(n_features)
        self.b = 0

        for _ in range(self.n_iters):
            for idx, x_i in enumerate(X):
                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1
                if condition:
                    self.w -= self.lr * (2 * self.lambda_param * self.w)
                else:
                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))
                    self.b -= self.lr * y_[idx]

    def predict(self, X):
        approx = np.dot(X, self.w) - self.b
        return np.sign(approx)
\end{lstlisting}

\subsection{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=SVM with scikit-learn]
from sklearn.svm import SVC

model = SVC(kernel='linear')  # or 'rbf', 'poly', etc.
model.fit(X_train, y_train)
predictions = model.predict(X_test)
\end{lstlisting}

\section{k-Nearest Neighbors (k-NN)}
\subsection{Explanation}

k-Nearest Neighbors is a simple, instance-based learning algorithm used for classification and regression. It classifies a data point based on how its neighbors are classified. The value of $k$ determines the number of neighbors to consider, and the choice of distance metric can impact performance. k-NN is intuitive and easy to implement but can be computationally intensive with large datasets. k-Nearest Neighbors makes the following assumptions:

\begin{enumerate}
    \item \textbf{Feature Relevance:} Assumes that all features are relevant and contribute equally to the distance metric.
    \item \textbf{Feature Scaling:} Assumes that features are scaled similarly since distance calculations are sensitive to the scale.
    \item \textbf{Local Consistency:} Assumes that similar instances are near each other in the feature space.
    \item \textbf{Sufficient Data:} Requires a sufficient amount of training data to provide accurate neighbors.
    \item \textbf{Noisy Data:} Sensitive to noisy data and outliers.
\end{enumerate}

\subsection{From Scratch}
\begin{lstlisting}[caption=k-NN from Scratch]
import numpy as np
from collections import Counter

class KNN:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def predict(self, X):
        predictions = []
        for x in X:
            distances = np.linalg.norm(self.X_train - x, axis=1)
            k_idx = distances.argsort()[:self.k]
            k_neighbor_labels = self.y_train[k_idx]
            most_common = Counter(k_neighbor_labels).most_common(1)
            predictions.append(most_common[0][0])
        return np.array(predictions)
\end{lstlisting}

\subsection{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=k-NN with scikit-learn]
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
\end{lstlisting}

\section{k-Means Clustering}
\subsection{Explanation}

k-Means is an unsupervised learning algorithm used for clustering data into $k$ distinct groups based on feature similarity. It partitions the dataset by minimizing the within-cluster sum of squares. k-Means is efficient and widely used but requires specifying the number of clusters in advance and can be sensitive to initial centroid placement. k-Means Clustering operates under the following assumptions:

\begin{enumerate}
    \item \textbf{Spherical Clusters:} Assumes that clusters are spherical and equally sized.
    \item \textbf{Feature Scaling:} Assumes that all features contribute equally to the distance calculations, necessitating feature scaling.
    \item \textbf{No Outliers:} Sensitive to outliers which can skew the centroid positions.
    \item \textbf{Initial Centroid Selection:} Assumes that initial centroids are chosen appropriately to avoid convergence to local minima.
    \item \textbf{Clusters are Convex:} Assumes that clusters are convex shapes in the feature space.
\end{enumerate}

\subsection{From Scratch}
\begin{lstlisting}[caption=k-Means Clustering from Scratch]
import numpy as np

class KMeans:
    def __init__(self, k=3, max_iters=100):
        self.k = k
        self.max_iters = max_iters

    def fit(self, X):
        self.centroids = X[np.random.choice(X.shape[0], self.k, replace=False)]
        for _ in range(self.max_iters):
            self.labels = self._assign_clusters(X)
            new_centroids = np.array([X[self.labels == i].mean(axis=0) for i in range(self.k)])
            if np.all(self.centroids == new_centroids):
                break
            self.centroids = new_centroids

    def _assign_clusters(self, X):
        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)
        return np.argmin(distances, axis=1)

    def predict(self, X):
        return self._assign_clusters(X)
\end{lstlisting}

\subsection{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=k-Means Clustering with scikit-learn]
from sklearn.cluster import KMeans

model = KMeans(n_clusters=3, max_iter=100)
model.fit(X_train)
labels = model.predict(X_test)
\end{lstlisting}


\section{Random Forest}
\subsection{Explanation}

Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It reduces overfitting by averaging multiple trees and is robust against noise and outliers. Random Forests are versatile and can handle large datasets with high dimensionality. Random Forests operate under the following assumptions:

\begin{enumerate}
    \item \textbf{Feature Independence:} Assumes that features are independent of each other.
    \item \textbf{No Assumptions About Data Distribution:} Unlike linear models, Random Forests do not assume any underlying distribution of the data.
    \item \textbf{Handle Non-linear Relationships:} Capable of capturing complex, non-linear relationships between features and the target.
    \item \textbf{Feature Scaling Not Required:} Random Forests are not sensitive to the scale of the features.
    \item \textbf{Sufficient Data and Diversity:} Requires a sufficient amount of data and diversity among trees to improve generalization.
    \item \textbf{No Overfitting (with Ensemble):} The ensemble nature helps in reducing overfitting compared to individual Decision Trees.
\end{enumerate}

\subsection{From Scratch}
\begin{lstlisting}[caption=Random Forest from Scratch]
import numpy as np
from collections import Counter

class DecisionTree:
    def __init__(self, max_depth=5, min_size=1):
        self.max_depth = max_depth
        self.min_size = min_size
        self.tree = None

    def fit(self, X, y):
        dataset = np.column_stack((X, y))
        self.tree = self._build_tree(dataset)

    def _build_tree(self, dataset, depth=0):
        X, y = dataset[:, :-1], dataset[:, -1]
        if len(set(y)) == 1 or depth >= self.max_depth:
            return {'type': 'leaf', 'value': self._most_common_label(y)}
        feature, threshold = self._best_split(X, y)
        if feature is None:
            return {'type': 'leaf', 'value': self._most_common_label(y)}
        left_idxs = X[:, feature] <= threshold
        right_idxs = X[:, feature] > threshold
        left = self._build_tree(dataset[left_idxs], depth + 1)
        right = self._build_tree(dataset[right_idxs], depth + 1)
        return {'type': 'node', 'feature': feature, 'threshold': threshold, 'left': left, 'right': right}

    def _best_split(self, X, y):
        best_gain = -1
        split_idx, split_thresh = None, None
        for feature in range(X.shape[1]):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                gain = self._information_gain(y, X[:, feature], threshold)
                if gain > best_gain:
                    best_gain = gain
                    split_idx = feature
                    split_thresh = threshold
        return split_idx, split_thresh

    def _information_gain(self, y, X_feat, thresh):
        parent_entropy = self._entropy(y)
        left_idxs = X_feat <= thresh
        right_idxs = X_feat > thresh
        if len(y[left_idxs]) == 0 or len(y[right_idxs]) == 0:
            return 0
        n = len(y)
        n_l, n_r = len(y[left_idxs]), len(y[right_idxs])
        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])
        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r
        return parent_entropy - child_entropy

    def _entropy(self, y):
        hist = np.bincount(y.astype(int))
        ps = hist / len(y)
        return -np.sum([p * np.log2(p) for p in ps if p > 0])

    def _most_common_label(self, y):
        return Counter(y).most_common(1)[0][0]

    def predict(self, X):
        return np.array([self._traverse_tree(x, self.tree) for x in X])

    def _traverse_tree(self, x, node):
        if node['type'] == 'leaf':
            return node['value']
        if x[node['feature']] <= node['threshold']:
            return self._traverse_tree(x, node['left'])
        return self._traverse_tree(x, node['right'])

class RandomForest:
    def __init__(self, n_trees=10, max_depth=5, min_size=1):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.min_size = min_size
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            idxs = np.random.choice(len(X), len(X), replace=True)
            X_sample, y_sample = X[idxs], y[idxs]
            tree = DecisionTree(max_depth=self.max_depth, min_size=self.min_size)
            tree.fit(X_sample, y_sample)
            self.trees.append(tree)

    def predict(self, X):
        tree_preds = np.array([tree.predict(X) for tree in self.trees])
        return [Counter(tree_preds[:,i]).most_common(1)[0][0] for i in range(len(X))]
\end{lstlisting}

\subsection{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Random Forest with scikit-learn]
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, max_depth=5)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
\end{lstlisting}

\section{Gradient Boosting}
\subsection{Explanation}
Gradient Boosting is an ensemble technique that builds models sequentially, where each new model attempts to correct the errors of the previous ones. It optimizes a loss function over function space by iteratively choosing a model that points in the negative gradient direction. Gradient Boosting is highly effective for both classification and regression tasks but can be prone to overfitting if not properly regularized. Gradient Boosting operates under the following assumptions:

\begin{enumerate}
    \item \textbf{Additive Model:} Assumes that the final model is a sum of weak learners (typically Decision Trees).
    \item \textbf{Loss Function Differentiability:} Requires a differentiable loss function to compute gradients.
    \item \textbf{Independence of Trees:} Each tree is built to correct the errors of the previous trees, assuming trees are not highly correlated.
    \item \textbf{No Strong Multicollinearity:} Assumes that features are not highly correlated, as it can lead to overfitting.
    \item \textbf{Sufficient Data:} Requires a large amount of data to capture complex patterns.
    \item \textbf{Feature Importance:} Assumes that the features contributing most to reducing the loss are the most important.
\end{enumerate}

\subsection{From Scratch}
\begin{lstlisting}[caption=Gradient Boosting from Scratch]
import numpy as np

class DecisionTree:
    def __init__(self, max_depth=3, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None

    def fit(self, X, y):
        self.tree = self._build_tree(X, y)

    def _build_tree(self, X, y, depth=0):
        num_samples, num_features = X.shape
        if depth >= self.max_depth or num_samples < self.min_samples_split:
            leaf_value = self._mean(y)
            return {'type': 'leaf', 'value': leaf_value}
        best_feature, best_threshold = self._best_split(X, y)
        if best_feature is None:
            leaf_value = self._mean(y)
            return {'type': 'leaf', 'value': leaf_value}
        left_idxs = X[:, best_feature] <= best_threshold
        right_idxs = X[:, best_feature] > best_threshold
        left = self._build_tree(X[left_idxs], y[left_idxs], depth + 1)
        right = self._build_tree(X[right_idxs], y[right_idxs], depth + 1)
        return {'type': 'node', 'feature': best_feature, 'threshold': best_threshold, 'left': left, 'right': right}

    def _best_split(self, X, y):
        best_mse = float('inf')
        split_idx, split_thresh = None, None
        for feature in range(X.shape[1]):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                left = y[X[:, feature] <= threshold]
                right = y[X[:, feature] > threshold]
                if len(left) == 0 or len(right) == 0:
                    continue
                mse = self._mse(left, right)
                if mse < best_mse:
                    best_mse = mse
                    split_idx = feature
                    split_thresh = threshold
        return split_idx, split_thresh

    def _mse(self, left, right):
        mse_left = np.var(left) * len(left)
        mse_right = np.var(right) * len(right)
        return mse_left + mse_right

    def _mean(self, y):
        return np.mean(y)

    def predict(self, X):
        return np.array([self._traverse_tree(x, self.tree) for x in X])

    def _traverse_tree(self, x, node):
        if node['type'] == 'leaf':
            return node['value']
        if x[node['feature']] <= node['threshold']:
            return self._traverse_tree(x, node['left'])
        return self._traverse_tree(x, node['right'])

class GradientBoosting:
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.trees = []
        self.initial_prediction = None

    def fit(self, X, y):
        # Initialize with mean prediction
        self.initial_prediction = np.mean(y)
        y_pred = np.full(y.shape, self.initial_prediction)
        
        for _ in range(self.n_estimators):
            residual = y - y_pred
            tree = DecisionTree(max_depth=self.max_depth)
            tree.fit(X, residual)
            update = tree.predict(X)
            y_pred += self.learning_rate * update
            self.trees.append(tree)

    def predict(self, X):
        y_pred = np.full((X.shape[0],), self.initial_prediction)
        for tree in self.trees:
            y_pred += self.learning_rate * tree.predict(X)
        return y_pred
\end{lstlisting}

\subsection{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Gradient Boosting with scikit-learn]
from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
\end{lstlisting}

\section{Neural Networks}

\subsection{Explanation}

Neural Networks are a class of machine learning models inspired by the human brain's structure. They consist of layers of interconnected neurons that process input data through weighted connections and activation functions. Neural Networks are highly flexible and can model complex, non-linear relationships, making them suitable for tasks such as image recognition, natural language processing, and more. However, they generally have a low level of interpretability (closer to black-box) and require substantial computational resources and data to train effectively.

\subsubsection{Assumptions}
\begin{itemize}
    \item \textbf{Linearity and Non-linearity:} Neural networks can model both linear and non-linear relationships.
    \item \textbf{Sufficient Data:} Requires large amounts of data to train effectively and avoid overfitting.
    \item \textbf{Independent Observations:} Assumes that training examples are independent of each other.
    \item \textbf{Feature Scaling:} Assumes that features are scaled appropriately for gradient-based optimization.
    \item \textbf{Appropriate Architecture:} Assumes that the network architecture (number of layers, neurons) is suitable for the problem.
\end{itemize}

\subsection{From Scratch}
\begin{lstlisting}[caption=Neural Network from Scratch]
import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, lr=0.01):
        # Initialize weights
        self.w1 = np.random.randn(input_size, hidden_size)
        self.b1 = np.zeros((1, hidden_size))
        self.w2 = np.random.randn(hidden_size, output_size)
        self.b2 = np.zeros((1, output_size))
        self.lr = lr

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def sigmoid_derivative(self, a):
        return a * (1 - a)

    def forward(self, X):
        self.z1 = np.dot(X, self.w1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.w2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2

    def backward(self, X, y, output):
        m = y.shape[0]
        delta2 = (output - y) * self.sigmoid_derivative(output)
        dw2 = np.dot(self.a1.T, delta2) / m
        db2 = np.sum(delta2, axis=0, keepdims=True) / m

        delta1 = np.dot(delta2, self.w2.T) * self.sigmoid_derivative(self.a1)
        dw1 = np.dot(X.T, delta1) / m
        db1 = np.sum(delta1, axis=0, keepdims=True) / m

        # Update weights
        self.w2 -= self.lr * dw2
        self.b2 -= self.lr * db2
        self.w1 -= self.lr * dw1
        self.b1 -= self.lr * db1

    def fit(self, X, y, epochs=1000):
        for _ in range(epochs):
            output = self.forward(X)
            self.backward(X, y, output)

    def predict(self, X):
        output = self.forward(X)
        return (output > 0.5).astype(int)
\end{lstlisting}

\subsection{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Neural Network with scikit-learn]
from sklearn.neural_network import MLPClassifier

model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=300)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
\end{lstlisting}

\section{Statistical Tests}

\subsection{t-Test}

\subsubsection{Explanation}
The t-test is used to determine if there is a significant difference between the means of two groups, which may be related in certain features. The t-Test operates under the following assumptions:

\begin{enumerate}
    \item \textbf{Independence of Observations:} The samples are independent of each other.
    \item \textbf{Normality:} The data in each group are approximately normally distributed.
    \item \textbf{Homogeneity of Variances (for equal variance t-test):} The variances in the two groups are equal.
    \item \textbf{Scale of Measurement:} The dependent variable is measured on an interval or ratio scale.
\end{enumerate}

\subsubsection{From Scratch}
\begin{lstlisting}[caption=t-Test from Scratch]
import numpy as np

def t_test(sample1, sample2):
    mean1, mean2 = np.mean(sample1), np.mean(sample2)
    var1, var2 = np.var(sample1, ddof=1), np.var(sample2, ddof=1)
    n1, n2 = len(sample1), len(sample2)
    se = np.sqrt(var1/n1 + var2/n2)
    t_stat = (mean1 - mean2) / se
    df = ((var1/n1 + var2/n2)**2) / ((var1**2)/((n1**2)*(n1-1)) + (var2**2)/((n2**2)*(n2-1)))
    return t_stat, df
\end{lstlisting}

\subsubsection{Using \texttt{scipy}}
\begin{lstlisting}[caption=t-Test with scipy]
from scipy import stats

t_stat, p_value = stats.ttest_ind(sample1, sample2, equal_var=False)
\end{lstlisting}

\subsection{Mann-Whitney U Test (u-Test)}

\subsubsection{Explanation}
The Mann-Whitney U test is a non-parametric test used to determine whether there is a difference between two independent samples. It does not assume normal distribution. The Mann-Whitney U test operates under the following assumptions:

\begin{enumerate}
    \item \textbf{Independent Samples:} The two samples are independent of each other.
    \item \textbf{Ordinal or Continuous Data:} The dependent variable should be ordinal or continuous.
    \item \textbf{Shape of Distributions:} Assumes that the distributions of both groups have the same shape.
\end{enumerate}

\subsubsection{From Scratch}
\begin{lstlisting}[caption=Mann-Whitney U Test from Scratch]
import numpy as np

def mann_whitney_u_test(sample1, sample2):
    combined = np.concatenate([sample1, sample2])
    ranks = np.argsort(np.argsort(combined)) + 1
    ranks1 = ranks[:len(sample1)]
    U1 = len(sample1)*len(sample2) + (len(sample1)*(len(sample1)+1))/2 - np.sum(ranks1)
    U2 = len(sample1)*len(sample2) - U1
    return U1, U2
\end{lstlisting}

\subsubsection{Using \texttt{scipy}}
\begin{lstlisting}[caption=Mann-Whitney U Test with scipy]
from scipy import stats

U, p = stats.mannwhitneyu(sample1, sample2, alternative='two-sided')
\end{lstlisting}

\subsection{z-Test}

\subsubsection{Explanation}
The z-test is used to determine whether there is a significant difference between sample and population means, or between two sample means, when the population variance is known.
\subsubsection{Assumptions}
\begin{itemize}
    \item \textbf{Independence:} The samples are independent of each other.
    \item \textbf{Normality:} The sampling distribution of the mean is normal (requires large sample sizes or known population variance).
    \item \textbf{Known Population Variance:} Assumes that the population variances are known.
    \item \textbf{Scale of Measurement:} The dependent variable should be measured on a continuous scale.
\end{itemize}
\subsubsection{From Scratch}
\begin{lstlisting}[caption=z-Test from Scratch]
import numpy as np

def z_test(sample1, sample2, sigma1, sigma2):
    mean1, mean2 = np.mean(sample1), np.mean(sample2)
    n1, n2 = len(sample1), len(sample2)
    z = (mean1 - mean2) / np.sqrt((sigma1**2)/n1 + (sigma2**2)/n2)
    return z
\end{lstlisting}

\subsubsection{Using \texttt{statsmodels}}
\begin{lstlisting}[caption=z-Test with statsmodels]
from statsmodels.stats.weightstats import ztest

z_stat, p_value = ztest(sample1, sample2, value=0, alternative='two-sided')
\end{lstlisting}

\subsection{Granger Causality Test}

\subsubsection{Explanation}
The Granger Causality test is used to determine whether one time series can predict another time series. It is based on the idea that if a signal $X$ Granger-causes a signal $Y$, then past values of $X$ should contain information that helps predict $Y$ above and beyond the information contained in past values of $Y$ alone.

\subsubsection{Assumptions}
\begin{itemize}
    \item \textbf{Stationarity:} The time series data should be stationary.
    \item \textbf{No Perfect Multicollinearity:} The explanatory variables should not be perfectly correlated.
    \item \textbf{Lag Order Selection:} The appropriate number of lags should be chosen for the test.
    \item \textbf{Linearity:} The relationship between the variables is linear.
\end{itemize}

\subsubsection{Using \texttt{statsmodels}}
\fromscratch Granger causality is complex to implement from scratch due to the need for vector autoregressive models. Therefore, using libraries like \texttt{statsmodels} is recommended.

\begin{lstlisting}[caption=Granger Causality Test with statsmodels]
import pandas as pd
from statsmodels.tsa.stattools import grangercausalitytests

data = pd.concat([series_X, series_Y], axis=1)
grangercausalitytests(data, maxlag=4)
\end{lstlisting}

\subsection{Chi-Squared Test}

\subsubsection{Explanation}
The Chi-Squared test is used to determine whether there is a significant association between two categorical variables. 

\subsubsection{Assumptions}
\begin{itemize}
    \item \textbf{Independence:} Observations are independent of each other.
    \item \textbf{Expected Frequency:} Each expected frequency in the contingency table should be at least 5.
    \item \textbf{Scale of Measurement:} Both variables should be categorical.
\end{itemize}

\subsubsection{From Scratch}
\begin{lstlisting}[caption=Chi-Squared Test from Scratch]
import numpy as np

def chi_squared_test(observed):
    observed = np.array(observed)
    row_sums = observed.sum(axis=1, keepdims=True)
    col_sums = observed.sum(axis=0, keepdims=True)
    total = observed.sum()
    expected = row_sums.dot(col_sums) / total
    chi2 = ((observed - expected)**2 / expected).sum()
    return chi2
\end{lstlisting}

\subsubsection{Using \texttt{scipy}}
\begin{lstlisting}[caption=Chi-Squared Test with scipy]
from scipy.stats import chi2_contingency

chi2, p, dof, expected = chi2_contingency(observed)
\end{lstlisting}

\subsection{ANOVA (Analysis of Variance)}

\subsubsection{Explanation}
ANOVA is used to compare the means of three or more samples to understand if at least one sample mean is different from the others.

\subsubsection{From Scratch}
\begin{lstlisting}[caption=ANOVA from Scratch]
import numpy as np

def anova_test(*groups):
    n = sum(len(group) for group in groups)
    overall_mean = np.mean(np.concatenate(groups))
    ss_between = sum(len(group)*(np.mean(group) - overall_mean)**2 for group in groups)
    ss_within = sum(np.sum((group - np.mean(group))**2) for group in groups)
    df_between = len(groups) - 1
    df_within = n - len(groups)
    ms_between = ss_between / df_between
    ms_within = ss_within / df_within
    F = ms_between / ms_within
    return F
\end{lstlisting}

\subsubsection{Using \texttt{scipy}}
\begin{lstlisting}[caption=ANOVA with scipy]
from scipy.stats import f_oneway

F_stat, p_value = f_oneway(group1, group2, group3)
\end{lstlisting}

\subsection{Correlation Test}

\subsubsection{Explanation}
Correlation tests assess the strength and direction of the linear relationship between two continuous variables.

\subsubsection{From Scratch}
\begin{lstlisting}[caption=Correlation Test from Scratch]
import numpy as np

def correlation(x, y):
    mean_x, mean_y = np.mean(x), np.mean(y)
    cov = np.sum((x - mean_x)*(y - mean_y))
    std_x = np.sqrt(np.sum((x - mean_x)**2))
    std_y = np.sqrt(np.sum((y - mean_y)**2))
    return cov / (std_x * std_y)
\end{lstlisting}

\subsubsection{Using \texttt{scipy}}
\begin{lstlisting}[caption=Correlation Test with scipy]
from scipy.stats import pearsonr

corr_coef, p_value = pearsonr(x, y)
\end{lstlisting}

\section{Evaluation Metrics}

\subsection{Accuracy}
\begin{lstlisting}[caption=Accuracy Calculation]
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_true, y_pred)
\end{lstlisting}

\subsection{Precision, Recall, F1-Score}
\begin{lstlisting}[caption=Precision, Recall, F1-Score Calculation]
from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
\end{lstlisting}

\subsection{Confusion Matrix}
\begin{lstlisting}[caption=Confusion Matrix]
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_true, y_pred)
\end{lstlisting}

\section{Data Preprocessing}

\subsection{Train-Test Split}
\begin{lstlisting}[caption=Train-Test Split]
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
\end{lstlisting}

\subsection{Standardization}
\begin{lstlisting}[caption=Standardization]
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
\end{lstlisting}

\section{Data Visualization and Model Evaluation}

Effective data visualization and model evaluation are crucial for understanding your data, diagnosing model performance, and communicating results. This section covers common visualization techniques and evaluation metrics with corresponding Python implementations.

\subsection{Data Visualization}

\subsubsection{Scatter Plot}

\paragraph{Explanation}
Scatter plots are used to visualize the relationship between two continuous variables. They help in identifying correlations, patterns, and potential outliers in the data.

\paragraph{Using \texttt{matplotlib}}
\begin{lstlisting}[caption=Scatter Plot with matplotlib]
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Scatter Plot of Feature 1 vs Feature 2')
plt.colorbar(label='Target')
plt.show()
\end{lstlisting}

\paragraph{Using \texttt{seaborn}}
\begin{lstlisting}[caption=Scatter Plot with seaborn]
import seaborn as sns
import pandas as pd

df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])
df['Target'] = y

sns.scatterplot(data=df, x='Feature 1', y='Feature 2', hue='Target', palette='viridis')
plt.title('Scatter Plot of Feature 1 vs Feature 2')
plt.show()
\end{lstlisting}

\subsubsection{Histogram}

\paragraph{Explanation}
Histograms display the distribution of a single continuous variable, showing the frequency of data points within specified bins.

\paragraph{Using \texttt{matplotlib}}
\begin{lstlisting}[caption=Histogram with matplotlib]
plt.figure(figsize=(8,6))
plt.hist(X[:, 0], bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Feature 1')
plt.ylabel('Frequency')
plt.title('Histogram of Feature 1')
plt.show()
\end{lstlisting}

\paragraph{Using \texttt{seaborn}}
\begin{lstlisting}[caption=Histogram with seaborn]
sns.histplot(X[:, 0], bins=30, kde=True, color='skyblue')
plt.xlabel('Feature 1')
plt.title('Histogram of Feature 1 with KDE')
plt.show()
\end{lstlisting}

\subsubsection{Heatmap (Correlation Matrix)}

\paragraph{Explanation}
Heatmaps visualize the correlation matrix of features, helping to identify multicollinearity and the strength of relationships between variables.

\paragraph{Using \texttt{seaborn}}
\begin{lstlisting}[caption=Correlation Heatmap with seaborn]
import numpy as np

correlation_matrix = np.corrcoef(X, rowvar=False)

sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm',
            xticklabels=['Feature '+str(i) for i in range(X.shape[1])],
            yticklabels=['Feature '+str(i) for i in range(X.shape[1])])
plt.title('Correlation Matrix Heatmap')
plt.show()
\end{lstlisting}

\subsection{Model Evaluation}

\subsubsection{Confusion Matrix Visualization}

\paragraph{Explanation}
A confusion matrix provides a summary of prediction results on a classification problem, showing the number of correct and incorrect predictions categorized by each class.

\paragraph{Using \texttt{seaborn}}
\begin{lstlisting}[caption=Confusion Matrix with seaborn]
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()
\end{lstlisting}

\subsubsection{ROC Curve and AUC}

\paragraph{Explanation}
The Receiver Operating Characteristic (ROC) curve illustrates the diagnostic ability of a binary classifier system by plotting the True Positive Rate (Recall) against the False Positive Rate at various threshold settings. The Area Under the Curve (AUC) quantifies the overall ability of the model to discriminate between classes.

\paragraph{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=ROC Curve and AUC with scikit-learn]
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()
\end{lstlisting}

\subsubsection{Precision-Recall Curve}

\paragraph{Explanation}
Precision-Recall curves are useful for evaluating binary classifiers, especially on imbalanced datasets. They plot Precision against Recall for different threshold values.

\paragraph{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Precision-Recall Curve with scikit-learn]
from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt

precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
average_precision = average_precision_score(y_true, y_scores)

plt.figure(figsize=(8,6))
plt.plot(recall, precision, color='blue', lw=2, label=f'AP = {average_precision:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='upper right')
plt.show()
\end{lstlisting}

\subsubsection{Feature Importance}

\paragraph{Explanation}
Feature importance plots indicate the contribution of each feature to the predictive power of the model. They are particularly useful for understanding which features are most influential in making predictions.

\paragraph{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Feature Importance with scikit-learn]
import matplotlib.pyplot as plt
import numpy as np

# Assuming 'model' is a trained tree-based model
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
features = ['Feature '+str(i) for i in range(X.shape[1])]

plt.figure(figsize=(10,6))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], color='lightblue', align='center')
plt.xticks(range(X.shape[1]), [features[i] for i in indices], rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance Score')
plt.show()
\end{lstlisting}

\subsubsection{Learning Curves}

\paragraph{Explanation}
Learning curves plot the training and validation performance of a model over varying sizes of the training dataset. They help in diagnosing whether a model is suffering from high bias (underfitting) or high variance (overfitting).

\paragraph{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Learning Curves with scikit-learn]
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np

train_sizes, train_scores, val_scores = learning_curve(model, X, y, 
                                                       train_sizes=np.linspace(0.1, 1.0, 10),
                                                       cv=5, scoring='accuracy')

train_mean = np.mean(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)

plt.figure(figsize=(8,6))
plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')
plt.plot(train_sizes, val_mean, 'o-', color='green', label='Validation Score')
plt.xlabel('Training Size')
plt.ylabel('Accuracy')
plt.title('Learning Curves')
plt.legend(loc='best')
plt.grid()
plt.show()
\end{lstlisting}

\subsubsection{Residual Plot}

\paragraph{Explanation}
Residual plots display the residuals (errors) of a model's predictions. They are useful for diagnosing non-linearity, unequal error variances, and outliers.

\paragraph{Using \texttt{matplotlib}}
\begin{lstlisting}[caption=Residual Plot with matplotlib]
import matplotlib.pyplot as plt

residuals = y_true - y_pred
plt.figure(figsize=(8,6))
plt.scatter(y_pred, residuals, alpha=0.7)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()
\end{lstlisting}

\subsubsection{Pair Plot}

\paragraph{Explanation}
Pair plots visualize pairwise relationships between features and the target variable. They are useful for identifying patterns and potential interactions between variables.

\paragraph{Using \texttt{seaborn}}
\begin{lstlisting}[caption=Pair Plot with seaborn]
import seaborn as sns
import pandas as pd

df = pd.DataFrame(X, columns=['Feature '+str(i) for i in range(X.shape[1])])
df['Target'] = y

sns.pairplot(df, hue='Target', palette='viridis')
plt.suptitle('Pair Plot of Features', y=1.02)
plt.show()
\end{lstlisting}

\subsubsection{Box Plot}

\paragraph{Explanation}
Box plots summarize the distribution of a dataset by displaying the median, quartiles, and potential outliers. They are useful for comparing distributions across different categories.

\paragraph{Using \texttt{seaborn}}
\begin{lstlisting}[caption=Box Plot with seaborn]
sns.boxplot(x='Target', y='Feature 1', data=df, palette='Set2')
plt.xlabel('Target')
plt.ylabel('Feature 1')
plt.title('Box Plot of Feature 1 by Target')
plt.show()
\end{lstlisting}

\subsection{Advanced Model Evaluation Techniques}

\subsubsection{Cross-Validation Results Visualization}

\paragraph{Explanation}
Visualizing cross-validation scores helps in understanding the variability and reliability of the model performance across different folds.

\paragraph{Using \texttt{seaborn}}
\begin{lstlisting}[caption=Cross-Validation Scores Visualization with seaborn]
import seaborn as sns
import matplotlib.pyplot as plt

cv_results = cross_val_score(model, X, y, cv=5, scoring='accuracy')
sns.boxplot(y=cv_results, color='lightgreen')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Scores')
plt.show()
\end{lstlisting}

\subsubsection{Cumulative Gains Chart}

\paragraph{Explanation}
Cumulative gains charts are used to evaluate the performance of classification models by showing the cumulative number of true positives captured as a function of the number of instances inspected.

\paragraph{Using \texttt{matplotlib}}
\begin{lstlisting}[caption=Cumulative Gains Chart with matplotlib]
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

# Assuming binary classification with y_true and y_scores
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_true, y_scores)

plt.figure(figsize=(8,6))
plt.plot(tpr, label='Model')
plt.plot([0,1], [0,1], linestyle='--', label='Random')
plt.xlabel('Percentage of Sample')
plt.ylabel('True Positive Rate')
plt.title('Cumulative Gains Chart')
plt.legend()
plt.show()
\end{lstlisting}

\subsubsection{Calibration Curve}

\paragraph{Explanation}
Calibration curves assess how well the predicted probabilities of a classifier are calibrated. A perfectly calibrated model will have predictions that match the observed probabilities.

\paragraph{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Calibration Curve with scikit-learn]
from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt

prob_true, prob_pred = calibration_curve(y_true, y_scores, n_bins=10)

plt.figure(figsize=(8,6))
plt.plot(prob_pred, prob_true, marker='o', label='Model')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')
plt.xlabel('Mean Predicted Probability')
plt.ylabel('Fraction of Positives')
plt.title('Calibration Curve')
plt.legend()
plt.show()
\end{lstlisting}

\subsubsection{Lift Chart}

\paragraph{Explanation}
Lift charts measure the effectiveness of a predictive model by comparing the results to a random selection. They show how much better the model is at predicting positives than random guessing.

\paragraph{Using \texttt{matplotlib}}
\begin{lstlisting}[caption=Lift Chart with matplotlib]
import matplotlib.pyplot as plt
import numpy as np

# Sort instances by predicted probability
sorted_indices = np.argsort(y_scores)[::-1]
y_true_sorted = y_true[sorted_indices]
cumulative_positive = np.cumsum(y_true_sorted)
lift = cumulative_positive / (np.arange(1, len(y_true)+1) * np.mean(y_true))

plt.figure(figsize=(8,6))
plt.plot(lift, label='Model Lift')
plt.plot([0, len(y_true)], [1, 1], linestyle='--', label='Random')
plt.xlabel('Number of Instances')
plt.ylabel('Lift')
plt.title('Lift Chart')
plt.legend()
plt.show()
\end{lstlisting}

\subsubsection{Residuals Distribution}

\paragraph{Explanation}
Analyzing the distribution of residuals helps in assessing the assumptions of regression models, such as homoscedasticity and normality of errors.

\paragraph{Using \texttt{seaborn}}
\begin{lstlisting}[caption=Residuals Distribution with seaborn]
import seaborn as sns

residuals = y_true - y_pred
sns.histplot(residuals, bins=30, kde=True, color='salmon')
plt.xlabel('Residuals')
plt.title('Residuals Distribution')
plt.show()
\end{lstlisting}

\subsubsection{Partial Dependence Plots (PDP)}

\paragraph{Explanation}
Partial Dependence Plots show the relationship between a subset of features and the predicted outcome, marginalizing over the values of all other features. They help in interpreting the effect of individual features on the model's predictions.

\paragraph{Using \texttt{scikit-learn}}
\begin{lstlisting}[caption=Partial Dependence Plot with scikit-learn]
from sklearn.inspection import plot_partial_dependence
import matplotlib.pyplot as plt

features = [0, 1]  # feature indices
plot_partial_dependence(model, X, features, grid_resolution=20)
plt.title('Partial Dependence Plots')
plt.show()
\end{lstlisting}

\subsubsection{SHAP Values for Feature Importance}

\paragraph{Explanation}
SHAP (SHapley Additive exPlanations) values provide a unified measure of feature importance by assigning each feature an importance value for a particular prediction. They offer deeper insights into how features contribute to model predictions.

\paragraph{Using \texttt{shap} Library}
\begin{lstlisting}[caption=SHAP Values with shap]
import shap

# Initialize the explainer
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test, feature_names=['Feature '+str(i) for i in range(X.shape[1])])
\end{lstlisting}


\end{document}
